\documentclass[../main.tex]{subfiles}

\begin{document}

\chapter{Introduction}

%Introduce the problem
A universal issue that affects software developers is the presence of software bugs. Bugs in software can be defined as unintentional errors that cause software to perform incorrectly. Defects in software can have a serious financial impact on businesses and reduce the time available for developers to create new features. Bugs in software can also introduce security vulnerabilities that could lead to the leakage of user data as well a loss of reputation \cite{briski2008minimizing} for an organisation. There have been several large-scale incidents in the past involving defective software. For example, the Mars Climate Orbiter, a space probe launched by NASA in 1998 crashed on Mars due to a bug which failed to use metric measurements instead of imperial units \cite{sauser2009projects}. Another example is the Heartbleed bug, a security vulnerability in the OpenSSL protocol which had the ability to expose confidential information such as passwords and credit card numbers \cite{durumeric2014matter}. In 1994, the Pentium FDIV bug incident caused Intel processors to produce faulty values when diving floating point values which lead to a mass recall of defective processors \cite{pratt1995anatomy}. These examples highlight some of the large scale impacts that bugs can have and it can be very challenging to identify these vulnerabilities in the first place. 

A study by IBM found that the associated cost with resolving a bug in software increases depending on what phase in the development lifecycle the bug was identified \cite{briski2008minimizing}. Moreover, software bugs that are discovered in the production phase are more costly because they can directly affect customers as opposed to bugs that occur early on in the development lifecycle. Therefore there is a justification for being able to detect bugs as early as possible. 

%What is currently being done today and limitations
Software developers identify bugs by performing regular testing and reduce the likelihood of new bugs appearing by improving development practices. For example, unit testing involves verifying the validity of individual methods within code and these tests and can be run every time a new build is created. Development practices have also shifted from traditional waterfall methods to Agile methodologies in order to keep up with the fast pace of development cycles. Agile methodologies allow for more frequent testing as well as code reviews before integrating new features to the main product. However, a challenge with identifying bugs is that as a project grows in size, it becomes more complex to maintain. This complexity makes it impractical to try out all possible executions of a program and running these tests can become time consuming for developers. 

%How can we solve it today
To assist developers with identifying bugs, recent research has suggested the usage of software defect prediction models \cite{kamei2013large}. These models aim to identify regions in code that are most vulnerable to defects. Using these models, developers would be able to prioritise their testing efforts, spend less time reviewing code and receive continuous feedback in an automated fashion. 

%Using machine learning to assist us
In order to create such a defect prediction model, one could leverage the capabilities of machine learning. Machine learning is the field of study concerned with algorithms that can learn from historical samples in order to predict values for unseen samples. Although machine learning has been around since the 60s, its viability has grown immensely in recent years due to advances in hardware, the open sourcing of machine learning frameworks and a large availability of data. Machine learning algorithms are able to spot complex patterns in data to solve various tasks such as facial recognition or translation of text and are now being tested out on the task of identifying bugs \cite{nayrolles2018clever}. In the context of identifying risky portions of code, one could use a supervised learning approach which involves showing a machine learning algorithm examples of risky and clean source files to allows the algorithm to predict whether a new file contains a defect. 

%Just-in-time defect prediction
A proposed method for identifying risky code is Just-in-time (JIT) defect prediction. Unlike previous research which has focused on predictions at the file-level, JIT defect prediction aims to predict individual commits as being risky or not \cite{kamei2013large}. The justification for these models is that commits tend to be smaller to review than files which can make the reviewing process simpler. Also, a problem with identifying risky files is that files can have multiple authors so it is unclear as to who should be assigned to resolve that file. Commits on the other hand only have one author so one could assign the commit's author to resolve any potential issues. Finally, JIT prediction models can make predictions rapidly because the one can extract the features required for a prediction as soon as the commit is made. 

%SZZ
When applying supervised learning techniques for this particular task, labelled data is required. The problem is that the majority of commits do not have any labels that indicate whether or not they caused a defect. In order to automatically label commits as \textit{risky} or \textit{not risky}, the Åšliwerski Zimmermann Zeller (SZZ) algorithm can be used to obtain large datasets for building defect prediction models \cite{sliwerski2005changes}. However, a limitation of the original algorithm is that it requires the project being analyzed to have a bug tracking database that confirms if certain commits resolve bugs. Some projects might not have this database or if they do, the information it stores might not contain data for a commit by commit basis. Instead, one could rely on the Approximate SZZ (ASZZ) algorithm which removes this restriction. 

%Semi-supervised learning
Since labelling commits can be quite time consuming, it would be useful to have a technique that could leverage the information contained in unlabelled commits. Semi-supervised learning is an example of a subset of machine learning algorithms that can train on both labelled and unlabelled data \cite{zhu2005semi}. The self-training algorithm is a simple semi-supervised technique that can be applied to any classification method that outputs probabilities for its labels \cite{zhu2007semi}. 

\section{Research Questions}

The purpose of this thesis is to investigate the performance of supervised classification models at the task of JIT defect prediction as well as to explore the viability of the self-training algorithm. A case study is also performed in order to understand how practical JIT defect prediction models are in an industrial environment such as King \footnote{https://king.com/}, a game development company. This thesis also performs a qualitative study to understand how software developers at King identify and resolve bugs. The research questions that will be answered are:

\begin{itemize}
  \item \textbf{RQ1:} What is the effectiveness of five notable machine learning classification algorithms at Just-in-time defect predictions?
  \item \textbf{RQ2:} When comparing the semi-supervised method of self-training with supervised learning techniques, how well does the semi-supervised approach perform when applied to Just-in-time defect predictions?
   \item \textbf{RQ3:} When provided with a Just-in-time defect prediction model, how accurate are the model's predictions according to software developers ? 
  \item \textbf{RQ4:} How do software developers identify bugs and what are some characteristics of code that contains a bug?
\end{itemize}

\section{Scope}

This project is focused on Just-in-time defect prediction models, models which predict at the commit level and only rely on commit metadata rather than analysing the source code. The collection of new datasets is done using the approximate SZZ algorithm and using a smaller set of features than those typically found in datasets for JIT defect prediction. 

\section{Outline of Report}
Chapter 2 provides relevant background theory on the field of software defect prediction as well as achievements in related work. Chapter 3 describes the technical contribution that was created for this thesis project. Chapter 4 covers the methodology utilised for answering the specified research questions. Chapter 5 presents the results of these experiments. Chapter 6 will analyse and discuss the key findings and chapter 7 will summarise the findings and suggest possiblities for future work in the field. 

\end{document}